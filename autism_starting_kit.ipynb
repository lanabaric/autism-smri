{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "<table style=\"width:100%; background-color:transparent;\">\n",
    "  <tr style=\"background-color:transparent;\">\n",
    "    <td style=\"background-color:transparent;\"><img href=\"http://www.datascience-paris-saclay.fr\" src=\"http://project.inria.fr/saclaycds/files/2017/02/logoUPSayPlusCDS_990.png\" ></td>\n",
    "    <td style=\"background-color:transparent;\"><img href=\"https://research.pasteur.fr/en/team/group-roberto-toro\" src=\"https://paris-saclay-cds.github.io/autism_challenge/images/institut_pasteur_logo.svg\" ></td>\n",
    "    <td style=\"background-color:black;\"><img href=\"fer.unizg.hr\" src=\"https://www.fer.unizg.hr/_pub/themes_static/fer2016/default/img/FER_logo.png\"></td>\n",
    "  </tr>\n",
    "</table> \n",
    "</div>\n",
    "\n",
    "<center><h1>Impact of sMRI preprocessing on autism classification using machine learning methods</h1></center>\n",
    "\n",
    "\n",
    "\n",
    "<center><h3>Forked from data challenge on Autism Spectrum Disorder detection</h3></center>\n",
    "<br/>\n",
    "<center>_Roberto Toro (Institut Pasteur), Nicolas Traut (Institut Pasteur), Anita Beggiato (Institut Pasteur), Katja Heuer (Institut Pasteur),<br /> Gael Varoquaux (Inria, Parietal), Alex Gramfort (Inria, Parietal), Balazs Kegl (LAL),<br /> Guillaume Lemaitre (CDS), Alexandre Boucaud (CDS), and Joris van den Bossche (CDS)<br />Lana Barić(FER), Roko Krstičević(FER)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "0. [Prerequisites](#Software-prerequisites)\n",
    "1. [Introduction about the competition](#Introduction:-what-is-this-challenge-about)\n",
    "3. [The data](#The-data)\n",
    "4. [Workflow](#Workflow)\n",
    "5. [Evaluation](#Evaluation)\n",
    "6. [Submission](#Submitting-to-the-online-challenge:-ramp.studio)\n",
    "7. [More information](#More-information)\n",
    "8. [Questions](#Question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To download and run this notebook**: download the [full starting kit](https://github.com/ramp-kits/autism/archive/master.zip), with all the necessary files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This starting kit requires the following dependencies:\n",
    "\n",
    "* `numpy`\n",
    "* `scipy`\n",
    "* `pandas`\n",
    "* `scikit-learn`\n",
    "* `matplolib`\n",
    "* `seaborn`\n",
    "* `nilearn`\n",
    "* `jupyter`\n",
    "* `ramp-workflow`\n",
    "\n",
    "The following 2 cells will install if necessary the missing dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install scikit-learn==0.21.3 seaborn==0.10.0 nilearn==0.7.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install `ramp-workflow` from the master branch on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install https://api.github.com/repos/paris-saclay-cds/ramp-workflow/zipball/0.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: detecting autism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autism spectrum disorder (ASD) is a developmental disorder affecting communication and behavior with different range in severity of symptoms. ASD has been reported to affect approximately 1 in 166 children.\n",
    "\n",
    "Although there is a consensus on a relation between ASD and atypical brain networks and anatomy, those differences in brain anatomy and functional connectivity remain unclear. To address these issues, study on large cohort of subjects are necessary to ensure relevant finding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "We start from downloading data from internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem import get_train_data\n",
    "\n",
    "data_train = get_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_train['participants_asd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of subjects in the training tests: {}'.format(data_train['participants_asd'].size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Participant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_participants = data_train[[col for col in data_train.columns if col.startswith('participants')]]\n",
    "data_train_participants.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structural MRI features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of structural features have been extracted for each subject: (i) normalized brain volume computed using subcortical segmentation of FreeSurfer and (ii) cortical thickness and area for right and left hemisphere of FreeSurfer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_anatomy = data_train[[col for col in data_train.columns if col.startswith('anatomy')]]\n",
    "data_train_anatomy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the column `anatomy_select` contain a label affected during a manual quality check (i.e. `0` and `3` reject, `1` accept, `2` accept with reserve). This column can be used during training to exclude noisy data for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_anatomy['anatomy_select'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The testing data can be loaded similarly as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem import get_test_data\n",
    "\n",
    "data_test = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_test['participants_asd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/workflow2.png\" width=\"100%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality selector\n",
    "\n",
    "Quality selector works by chosing some amount of bad quality data from the data set based on input of quality that ranges from 0 to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from problem import get_train_data\n",
    "\n",
    "data_train = get_train_data()\n",
    "\n",
    "def quality_selector(data, quality_factor = 1):\n",
    "\n",
    "    # Define variables and claer them\n",
    "    bad_data = pd.DataFrame()\n",
    "    decent_data = pd.DataFrame()\n",
    "    good_data = pd.DataFrame()\n",
    "    tmp_data = pd.DataFrame()\n",
    "\n",
    "    bad_quality = 1 - min(2 * quality_factor, 1)\n",
    "    decent_quality = quality_factor * 2\n",
    "    if quality_factor > 0.5:\n",
    "        decent_quality = 2 - 2 * quality_factor\n",
    "    good_quality = 0\n",
    "    if quality_factor > 0.5:\n",
    "        good_quality = quality_factor * 2 - 1\n",
    "\n",
    "    # First we get all the data with anatomy_select = 0 or 3\n",
    "    bad_data = data[data['anatomy_select'].isin([0, 3])]\n",
    "\n",
    "    # Then we get all the data with anatomy_select = 2\n",
    "    decent_data = data[data['anatomy_select'] == 2]\n",
    "\n",
    "    # Then we get all the data with anatomy_select = 1\n",
    "    good_data = data[data['anatomy_select'] == 1]\n",
    "\n",
    "    # Now we select the amount of data according to quality_factor\n",
    "    if bad_quality > 0:\n",
    "        bad_data = bad_data.sample(frac=bad_quality, replace=True)\n",
    "    if decent_quality > 0:\n",
    "        decent_data = decent_data.sample(frac=decent_quality, replace=True)\n",
    "    if good_quality > 0:\n",
    "        good_data = good_data.sample(frac=good_quality, replace=True)\n",
    "\n",
    "    if bad_quality == 0:\n",
    "        bad_data = pd.DataFrame()\n",
    "        print(\"Amount of bad quality images: 0\")\n",
    "    else:\n",
    "        print(f\"Amount of bad decent images: {bad_data['participants_asd'].size}\")\n",
    "    if decent_quality == 0:\n",
    "        decent_data = pd.DataFrame()\n",
    "        print(\"Amount of decent quality images: 0\")\n",
    "    else:\n",
    "        print(f\"Amount of decent images: {decent_data['participants_asd'].size}\")\n",
    "    if good_quality == 0:\n",
    "        good_data = pd.DataFrame()\n",
    "        print(\"Amount of good quality images: 0\")\n",
    "    else:\n",
    "        print(f\"Amount of good images: {good_data['participants_asd'].size}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Finally we concatenate the data\n",
    "    dataframes = [df for df in [bad_data, decent_data, good_data] if not df.empty]\n",
    "    if not dataframes:\n",
    "        raise ValueError('All sampled data is empty. Try a larger quality_factor.')\n",
    "    tmp_data = pd.concat(dataframes)\n",
    "\n",
    "    tmp_data['anatomy_select'] = 1\n",
    "\n",
    "    return tmp_data\n",
    "\n",
    "\n",
    "a = quality_selector(data_train, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The framework is evaluated with a cross-validation approach. The metrics used are the AUC under the ROC and the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from problem import get_cv\n",
    "\n",
    "def evaluation(X, y):\n",
    "    pipe = make_pipeline(FeatureExtractor(), Classifier())\n",
    "    cv = get_cv(X, y)\n",
    "    results = cross_validate(pipe, X, y, scoring=['roc_auc', 'accuracy'], cv=cv,\n",
    "                             verbose=1, return_train_score=True,\n",
    "                             n_jobs=1)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The available structural data can be used directly to make some classification. In this regard, we will use a feature extractor (i.e. `FeatureExtractor`). This extractor will only select only the anatomical features, dropping any information regarding the fMRI-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X_df, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        # get only the anatomical information\n",
    "        X = X_df[[col for col in X_df.columns if col.startswith('anatomy')]]\n",
    "        return X.drop(columns='anatomy_select')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose to use a logistic classifier preceded from a scaler which will remove the mean and standard deviation computed on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.clf = make_pipeline(StandardScaler(), LogisticRegression(solver='liblinear'))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf.fit(X, y)\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.clf.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.clf.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test locally our pipeline using `evaluation` function that we defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from problem import get_train_data\n",
    "\n",
    "data_train = get_train_data()\n",
    "\n",
    "data_train = quality_selector(data_train, 0.8)\n",
    "\n",
    "labels_train = data_train['participants_asd']\n",
    "\n",
    "results = evaluation(data_train.drop('participants_asd', axis=1), labels_train)\n",
    "\n",
    "print(\"Training score ROC-AUC: {:.3f} +- {:.3f}\".format(np.mean(results['train_roc_auc']),\n",
    "                                                        np.std(results['train_roc_auc'])))\n",
    "print(\"Validation score ROC-AUC: {:.3f} +- {:.3f} \\n\".format(np.mean(results['test_roc_auc']),\n",
    "                                                          np.std(results['test_roc_auc'])))\n",
    "\n",
    "print(\"Training score accuracy: {:.3f} +- {:.3f}\".format(np.mean(results['train_accuracy']),\n",
    "                                                         np.std(results['train_accuracy'])))\n",
    "print(\"Validation score accuracy: {:.3f} +- {:.3f}\".format(np.mean(results['test_accuracy']),\n",
    "                                                           np.std(results['test_accuracy'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the [README](https://github.com/paris-saclay-cds/ramp-workflow/blob/master/README.md) of the [ramp-workflow library](https://github.com/paris-saclay-cds/ramp-workflow)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
